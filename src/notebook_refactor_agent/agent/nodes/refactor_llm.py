from __future__ import annotations

from pathlib import Path
from typing import Any, cast

import nbformat

from ...llm.factory import create_llm
from ...llm.json_utils import extract_json
from ...plan import FunctionSpec, Plan


def _default_module_content() -> str:
    """Very small, PEP8/Black-valid default module that safely no-ops."""
    return """\
\"\"\"Default module generated by NotebookRefactorAgent.

This is a fallback implementation used when the LLM did not provide any files.
\"\"\"

from __future__ import annotations


def run_all() -> None:
    \"\"\"Optional entry point â€” does nothing by default.\"\"\"
    return None
"""


def _default_tests_content(module_rel: str) -> str:
    """
    Default pytest file that imports the module by its relative path and calls run_all()
    if present. It only asserts that import & optional call succeed.
    """
    return f"""\
import importlib.util
import pathlib


def test_import_and_run() -> None:
    root = pathlib.Path(__file__).resolve().parents[1]
    p = (root / pathlib.Path({module_rel!r})).resolve()
    spec = importlib.util.spec_from_file_location("generated.module", p)
    assert spec is not None and spec.loader is not None
    m = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(m)
    fn = getattr(m, "run_all", None)
    if callable(fn):
        fn()
"""


def refactor_llm_node(state: dict[str, Any]) -> dict[str, Any]:
    """
    Refactor node backed by an LLM. It:
      1) Extracts minimal code fragments from the notebook for context.
      2) Calls the LLM to produce files.
      3) Falls back to writing a minimal module & test when the LLM returns nothing
         (or omits the expected paths).
    """
    input_nb = Path(state["input_nb"])
    nb: Any = cast(Any, nbformat.read(str(input_nb), as_version=4))

    # plan is a Plan dataclass now
    plan: Plan = cast(Plan, state["plan"])
    functions: list[FunctionSpec] = plan.functions or []

    # Map code-cell ordinals (0..N-1) to actual nb cell indices
    code_cell_indices = [i for i, c in enumerate(nb.cells) if c.get("cell_type") == "code"]

    # Collect original code fragments per planned function
    frags: dict[int, str] = {}
    for spec in functions:
        cid = int(spec.cell_id)
        # Prefer mapping via code-cell ordinal; fall back to raw index if it fits
        if 0 <= cid < len(code_cell_indices):
            nb_idx = code_cell_indices[cid]
        elif 0 <= cid < len(nb.cells):
            nb_idx = cid
        else:
            continue
        src = str(nb.cells[nb_idx].get("source", ""))
        frags[cid] = src

    provider = str(state.get("provider", "none"))
    model = str(state.get("model", "none"))
    temperature = float(state.get("temperature", 0.1))
    max_tokens = int(state.get("max_output_tokens", 2048))

    llm = create_llm(provider)
    messages = [
        {
            "role": "system",
            "content": (
                "You are the **Refactor Agent**. You will turn small code fragments into a clean "
                "Python module and minimal tests. Return JSON with keys: module_path, tests_path, "
                "package_root, and files (a map of path->content)."
            ),
        },
        {
            "role": "user",
            "content": (
                "Here are code fragments by function (in source order):\n"
                + "\n".join(
                    f"### function {spec.fn_name} (cell {spec.cell_id})\n"
                    f"{frags.get(int(spec.cell_id), '')}"
                    for spec in functions
                )
                + "\n\nConstraints: minimal dependencies, PEP8, add simple docstrings."
            ),
        },
    ]

    text, meta = llm.chat(messages, model=model, temperature=temperature, max_tokens=max_tokens)
    obj = extract_json(text)

    # Resolve desired paths using LLM output with fallback to the plan
    package_root = str(obj.get("package_root", plan.package_root or "src_pkg"))
    module_path = str(obj.get("module_path", plan.module_path or f"{package_root}/module.py"))
    tests_path = str(obj.get("tests_path", plan.tests_path or "tests/test_module.py"))

    # Files returned by LLM (may be empty or partial)
    files = cast(dict[str, str], obj.get("files", {}) or {})

    # Ensure we *always* produce the expected module & tests as a fallback.
    if module_path not in files:
        files[module_path] = _default_module_content()
    if tests_path not in files:
        files[tests_path] = _default_tests_content(module_path)

    # Expose resolved paths for later reporting/steps
    artifacts = cast(dict[str, Any], state.get("artifacts", {}) or {})
    artifacts.update(
        {"package_root": package_root, "module_path": module_path, "tests_path": tests_path}
    )
    state["artifacts"] = artifacts

    # Write files
    out_dir = Path(state["output_dir"])
    for rel, content in files.items():
        p = (out_dir / rel).resolve()
        p.parent.mkdir(parents=True, exist_ok=True)
        p.write_text(content)

    # Persist useful reporting/metrics
    calls = cast(list[dict[str, Any]], state.get("llm_calls", []) or [])
    calls.append(
        {
            "node": "refactor_llm",
            "provider": provider,
            "model": model,
            "meta": meta,
        }
    )

    report = (
        f"Refactor complete. Wrote {len(files)} files.\n"
        f"Module: {(out_dir / module_path).resolve()}\n"
        f"Tests:  {(out_dir / tests_path).resolve()}"
    )
    state.update(
        {
            "report": report,
            "plan": plan,  # keep the Plan in state
            "llm_calls": calls,
        }
    )
    return state
